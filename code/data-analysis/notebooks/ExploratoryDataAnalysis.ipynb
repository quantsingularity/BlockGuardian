{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlockGuardian - Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs an initial exploratory data analysis on the transaction and fraud pattern datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Adjust matplotlib settings for better visualizations\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette('pastel')\n",
    "\n",
    "# Import the custom loader script (assuming it's in ../scripts/)\n",
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "from load_preprocess import load_data, preprocess_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets\n",
    "\n",
    "We will load the `transaction_history.csv` and `fraud_patterns.csv` datasets. These are currently placeholders or example datasets located in `resources/datasets/`. In a real scenario, these would be actual project data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the datasets (relative to the project root or data-analysis directory based on execution context)\n",
    "# For this notebook, we assume data is in ../../resources/datasets/ or a similar accessible path.\n",
    "# If running from data-analysis/notebooks, then ../../resources/datasets/ is correct for project structure.
",
    "# If the load_preprocess.py script expects data in data-analysis/data, we might need to copy it there first or adjust paths.
",
    "# For now, let's assume the load_preprocess script handles pathing or we place data in data-analysis/data
",
    "transaction_data_path = \"../../resources/datasets/transaction_history.csv\" # Original path
",
    "fraud_patterns_data_path = \"../../resources/datasets/fraud_patterns.csv\" # Original path
",
    "
",
    "# Let's try to use the paths from the resources directory first
",
    "# If these files don't exist, the load_data function in load_preprocess.py might create dummy data if we modify it or run it directly.
",
    "# The current load_preprocess.py creates dummy data in ../data/ relative to its own location (i.e. data-analysis/data)
",
    "
",
    "# Ensure dummy data exists if real data is not present
",
    "try:
",
    "    df_transactions_raw = load_data(transaction_data_path)
",
    "except FileNotFoundError:
",
    "    print(f\"File not found: {transaction_data_path}. Trying data-analysis/data/transaction_history.csv\")
",
    "    transaction_data_path_alt = \"../data/transaction_history.csv\" # Path relative to this notebook for data-analysis/data
",
    "    df_transactions_raw = load_data(transaction_data_path_alt)
",
    "    if df_transactions_raw is None:
",
    "        print(f\"Attempting to run load_preprocess.py to generate dummy data...\")
",
    "        # This is a bit of a hack for a notebook; ideally, data prep is a separate step.
",
    "        import subprocess
",
    "        subprocess.run([sys.executable, '../scripts/load_preprocess.py'])
",
    "        df_transactions_raw = load_data(transaction_data_path_alt)
",
    "
",
    "try:
",
    "    df_fraud_patterns_raw = load_data(fraud_patterns_data_path)
",
    "except FileNotFoundError:
",
    "    print(f\"File not found: {fraud_patterns_data_path}. Trying data-analysis/data/fraud_patterns.csv\")
",
    "    fraud_patterns_data_path_alt = \"../data/fraud_patterns.csv\" # Path relative to this notebook for data-analysis/data
",
    "    df_fraud_patterns_raw = load_data(fraud_patterns_data_path_alt)
",
    "    # Create dummy fraud_patterns if not found
",
    "    if df_fraud_patterns_raw is None:
",
    "        print(f\"Creating dummy fraud_patterns.csv at {fraud_patterns_data_path_alt}\")
",
    "        dummy_fraud_df = pd.DataFrame({
",
    "            'pattern_id': ['P001', 'P002', 'P003'],
",
    "            'description': ['High frequency small transactions', 'Large withdrawal after long inactivity', 'Transaction to known malicious address'],
",
    "            'severity': ['Medium', 'High', 'Critical']
",
    "        })
",
    "        dummy_fraud_df.to_csv(fraud_patterns_data_path_alt, index=False)
",
    "        df_fraud_patterns_raw = load_data(fraud_patterns_data_path_alt)
",
    "
",
    "if df_transactions_raw is not None:
",
    "    print(\"Transaction Data Head:\")
",
    "    print(df_transactions_raw.head())
",
    "    print(\"\nTransaction Data Info:\")
",
    "    df_transactions_raw.info()
",
    "else:
",
    "    print(\"Failed to load transaction data.\")
",
    "
",
    "if df_fraud_patterns_raw is not None:
",
    "    print(\"\nFraud Patterns Data Head:\")
",
    "    print(df_fraud_patterns_raw.head())
",
    "    print(\"\nFraud Patterns Data Info:\")
",
    "    df_fraud_patterns_raw.info()
",
    "else:
",
    "    print(\"Failed to load fraud patterns data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data\n",
    "\n",
    "Apply the preprocessing steps defined in `load_preprocess.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions_processed = preprocess_data(df_transactions_raw.copy() if df_transactions_raw is not None else None)
",
    "# df_fraud_patterns_processed = preprocess_data(df_fraud_patterns_raw.copy()) # Assuming similar preprocessing if needed
",
    "
",
    "if df_transactions_processed is not None:
",
    "    print(\"\nProcessed Transaction Data Head:\")
",
    "    print(df_transactions_processed.head())
",
    "else:
",
    "    print(\"Transaction data preprocessing failed or skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis - Transactions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_transactions_processed is not None:
",
    "    # Summary statistics
",
    "    print(\"\nSummary Statistics for Transactions:\")
",
    "    print(df_transactions_processed.describe(include='all'))
",
    "
",
    "    # Distribution of transaction amounts
",
    "    plt.figure(figsize=(10, 6))
",
    "    sns.histplot(df_transactions_processed['amount'], kde=True, bins=30)
",
    "    plt.title('Distribution of Transaction Amounts')
",
    "    plt.xlabel('Amount')
",
    "    plt.ylabel('Frequency')
",
    "    plt.show()
",
    "
",
    "    # Count of fraudulent vs. non-fraudulent transactions
",
    "    if 'fraud_flag' in df_transactions_processed.columns:
",
    "        plt.figure(figsize=(7, 5))
",
    "        sns.countplot(x='fraud_flag', data=df_transactions_processed)
",
    "        plt.title('Count of Fraudulent vs. Non-Fraudulent Transactions')
",
    "        plt.xlabel('Fraud Flag (1=Fraud, 0=Non-Fraud)')
",
    "        plt.ylabel('Count')
",
    "        plt.show()
",
    "        print(df_transactions_processed['fraud_flag'].value_counts(normalize=True))
",
    "    else:
",
    "        print(\"'fraud_flag' column not found in the transaction data.\")
",
    "
",
    "    # Transactions over time (if timestamp is available and correctly parsed)
",
    "    if 'timestamp' in df_transactions_processed.columns:
",
    "        # Ensure timestamp is datetime
",
    "        df_transactions_processed['timestamp'] = pd.to_datetime(df_transactions_processed['timestamp'])
",
    "        df_transactions_processed.set_index('timestamp', inplace=True)
",
    "
",
    "        plt.figure(figsize=(14, 7))
",
    "        df_transactions_processed['amount'].resample('D').sum().plot()
",
    "        plt.title('Total Transaction Amount Over Time (Daily)')
",
    "        plt.xlabel('Date')
",
    "        plt.ylabel('Total Amount')
",
    "        plt.show()
",
    "
",
    "        # Reset index if it was set for resampling, to avoid issues later
",
    "        df_transactions_processed.reset_index(inplace=True)
",
    "    else:
",
    "        print(\"'timestamp' column not found or not in datetime format.\")
",
    "else:
",
    "    print(\"Skipping EDA for transactions as data is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Placeholder for Model Training/Evaluation Script Call\n",
    "\n",
    "This section would typically involve calling a script that trains a fraud detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:
",
    "# print(\"\n--- Placeholder for Model Training ---\")
",
    "# import subprocess
",
    "# try:
",
    "#     # Assuming you have a train_model.py script in ../scripts/
",
    "#     subprocess.run([sys.executable, '../scripts/train_model.py', '--data_path', '../data/processed_transactions.csv'], check=True)
",
    "#     print(\"Model training script executed successfully (placeholder).\")
",
    "# except subprocess.CalledProcessError as e:
",
    "#     print(f\"Error during model training script execution: {e}\")
",
    "# except FileNotFoundError:
",
    "#     print(\"train_model.py not found. Skipping model training placeholder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Further Analysis Ideas\n",
    "\n",
    "- Correlation analysis between features.\n",
    "- User-level transaction patterns.\n",
    "- Anomaly detection techniques beyond simple fraud flags.\n",
    "- Feature engineering for model improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
